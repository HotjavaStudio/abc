{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REINFORCE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjFnHdUwd27CigkjSvPxSQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KyeongHoSeong/abc/blob/main/REINFORCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9LPtZAIExl0"
      },
      "source": [
        "# 정책 기울기\n",
        "reference: https://towardsdatascience.com/policy-gradient-reinforce-algorithm-with-baseline-e95ace11c1c4\n",
        "\n",
        "## REINFORCE\n",
        "정책 그라데이션 방법은 직접적으로 매개 변수화하여 정책을 배운다\n",
        "\n",
        "<img src =\"https://miro.medium.com/max/258/0*FyVhASxbAyPKGl0Y\" />\n",
        "\n",
        "그러나 최적화에 관해서는 여전히 값 함수 V (θ) 를 목적 함수로 사용해야 한다.\n",
        "\n",
        "목표: 정책 π에 따라 궤적 τ 에서 예상되는 총 보상 인 V (θ) 를 최대화\n",
        "\n",
        "value function V(θ) is calculated from the parameterized policy π(θ), not directly parameterized by θ.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/653/1*fu9HGx2iEqvh89-LyCs3VA.png\" />\n",
        "\n",
        " τ는 상태-행동 궤적\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/596/1*1G97-c15iPXFPiWuR2nzRQ.png\" />\n",
        "\n",
        "목표는 V (θ) 를 최대화하는 정책에 대한 매개 변수 θ를 찾는 것\n",
        "\n",
        "이를 위해 wrt 매개 변수 θ에 대하여, 정책의 기울기를 오름차순으로 V(θ)에서 최대 값을 검색합니다 .\n",
        "\n",
        "<img src =\"https://miro.medium.com/max/518/1*Vmmy0Aq-owGqYvldSE9PPw.png\" />\n",
        "\n",
        "정책 π (θ) 는 일반적으로 소프트 맥스, 가우스 또는 신경망을 사용하여 모델링되어 미분 가능합니다.\n",
        "\n",
        "여기서 우리는 그라디언트를 계산하기 위해 시간적 차이를 활용하는 바닐라 정책 그라디언트: REINFORCE의 인기있는 변형을 구현합니다. \n",
        "\n",
        "<img src =\"https://miro.medium.com/max/525/1*LzsbTMrlsKhxNIKeqUsOXw.png\" />\n",
        "\n",
        "\n",
        "## training procedure\n",
        "<img src=\"https://miro.medium.com/max/755/1*MhgPkwPnEN2ytvN9mLRxfA.png\" />\n",
        "\n",
        "    loop all episodes\n",
        "      loop episode in episodes\n",
        "        for step in episode\n",
        "          Update maximize (gradient of MSE(policy function)) \n",
        "\n",
        " ## Policy Gradient with Baseline\n",
        "정책 기울기 방법의 한 가지 부정적인 점은 경험적 수익으로 인한 높은 분산.\n",
        "- 분산을 줄이는 일반적인 방법 은 정책 기울기의 수익률에서 기준선 b (s) 를 빼는 것 \n",
        "- 기준선은 본질적으로 예상되는 실제 수익에 대한 프록시이며 정책 기울기에 편향을 도입해서는 안됩니다. \n",
        "- 실제로 가치 함수 자체는 기준선에 적합한 후보입니다. 기준선을 뺀 후 얻는 새로운 용어는 advantage A_t 로 정의 됩니다.\n",
        "\n",
        "<img src =\"https://miro.medium.com/max/610/1*j4yoEArIPogOiS96mvICtQ.png\" />\n",
        "\n",
        "- 참고로 또 다른 인기있는 Policy Gradient 변형 인 Actor-critic 메서드가 있습니다.이 메서드 는 경험적 반환 G_t 를 사용하는 대신 다른 매개 변수화 된 모델 Q (s, a) 를 사용하여 이점의 값을 근사화합니다 . 이는 또한 편향이 증가하는 대신 분산을 줄이는 데 도움이됩니다.\n",
        "\n",
        "baseline은 매개 변수화 된 값 함수로, 경험적 기대 수익률과 기준선 예측의 평균 제곱 오차를 줄임으로써 학습 할 수 있습니다.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/425/1*-deBYHAB5na7tZnYW6twLQ.png\" />\n",
        "\n",
        "기준을 사용하면 새로운 교육 루프에 advantage을 계산하고 baseline은 모델을 업데이트하는 두 가지 추가 단계가 있습니다.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/841/1*6065jhiJnZ3EcnPt7TfvdQ.png\" />\n",
        "\n",
        "# Python 구현 (Tensorflow 2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skLJeKjVMDQ1"
      },
      "source": [
        "## 1. Policy Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grzzjunnEw_n"
      },
      "source": [
        "import tensorflow_probability as tfp\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "class PolicyNet():\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.model = keras.Sequential(\n",
        "            layers=[\n",
        "                keras.Input(shape=(input_size,)),\n",
        "                layers.Dense(64, activation=\"relu\", name=\"relu_layer\"),\n",
        "                layers.Dense(output_size, activation=\"linear\", name=\"linear_layer\")\n",
        "            ],\n",
        "            name=\"policy\")\n",
        "\n",
        "    def action_distribution(self, observations):\n",
        "        logits = self.model(observations)\n",
        "        return tfp.distributions.Categorical(logits=logits)\n",
        "\n",
        "    def sampel_action(self, observations):\n",
        "        sampled_actions = self.action_distribution(observations).sample().numpy()\n",
        "        return sampled_actions"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHsYddQrMgUA"
      },
      "source": [
        "## 2.  baseline network\n",
        "\n",
        "Value function을 baseline으로 사용\n",
        "- input: current state\n",
        "- output: predicted value V(s)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-xHB2wMMJ0G"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "class BaselineNet():\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.model = keras.Sequential(\n",
        "            layers=[\n",
        "                keras.Input(shape=(input_size,)),\n",
        "                layers.Dense(64, activation=\"relu\", name=\"relu_layer\"),\n",
        "                layers.Dense(output_size, activation=\"linear\", name=\"linear_layer\")\n",
        "            ],\n",
        "            name=\"baseline\")\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=3e-2)\n",
        "\n",
        "    def forward(self, observations):\n",
        "        output = tf.squeeze(self.model(observations))\n",
        "        return output\n",
        "\n",
        "    # target: 관찰된 평균 보상(returns) <-게임을 하면서 수집된 값\n",
        "    # 기준선 네트워크는 수익률returns  및 예측forecasts의 \n",
        "    # 평균 제곱 오차를 최소화하여 업데이트됩니다.\n",
        "    def update(self, observations, target):\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.forward(observations)\n",
        "            loss = tf.keras.losses.mean_squared_error(y_true=target, y_pred=predictions)\n",
        "        grads = tape.gradient(loss, self.model.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufWGKksONztT"
      },
      "source": [
        "## 3.  PolicyGradient class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auyFiORRQlYD"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from gym import wrappers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from model.baseline_net import BaselineNet\n",
        "# from model.policy_net import PolicyNet\n",
        "\n",
        "\n",
        "def export_plot(ys, ylabel, title, filename):\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(ys)), ys)\n",
        "    plt.xlabel(\"Training Episode\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "class PolicyGradient(object):\n",
        "    # 정의한 정책 및 기본 네트워크를 포함, 모델에 필요한 모든 매개 변수를 초기화\n",
        "    def __init__(self, env, num_iterations=300, batch_size=2000, max_ep_len=200, output_path=\"../results/\"):\n",
        "        self.output_path = output_path\n",
        "        if not os.path.exists(output_path):\n",
        "            os.makedirs(output_path)\n",
        "        self.env = env\n",
        "        self.observation_dim = self.env.observation_space.shape[0]\n",
        "        self.action_dim = self.env.action_space.n\n",
        "        self.gamma = 0.99\n",
        "        self.num_iterations = num_iterations\n",
        "        self.batch_size = batch_size\n",
        "        self.max_ep_len = max_ep_len\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=3e-2)\n",
        "        self.policy_net = PolicyNet(input_size=self.observation_dim, output_size=self.action_dim)\n",
        "        self.baseline_net = BaselineNet(input_size=self.observation_dim, output_size=1)\n",
        "\n",
        "    # we can play several roll-outs to collect data.\n",
        "    # 1. sample an action from the policy network\n",
        "    # 2. paly a step to get reward and next state\n",
        "    # 3. at the end of episode, we collect all game trajectories \n",
        "    # consisting of states, actions, and rewards\n",
        "    # * 이 궤적은 정책 네트워크와 baseline network를 업데이트하는데 사용된다.\n",
        "    def play_games(self, env=None, num_episodes = None):\n",
        "        episode = 0\n",
        "        episode_rewards = []\n",
        "        paths = []\n",
        "        t = 0\n",
        "        if not env:\n",
        "            env = self.env\n",
        "\n",
        "        while (num_episodes or t < self.batch_size):\n",
        "            state = env.reset()\n",
        "            states, actions, rewards = [], [], []\n",
        "            episode_reward = 0\n",
        "\n",
        "            for step in range(self.max_ep_len):\n",
        "                states.append(state)\n",
        "                action = self.policy_net.sampel_action(np.atleast_2d(state))[0]\n",
        "                state, reward, done, _ = env.step(action)\n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                episode_reward += reward\n",
        "                t += 1\n",
        "\n",
        "                if (done or step == self.max_ep_len-1):\n",
        "                    episode_rewards.append(episode_reward)\n",
        "                    break\n",
        "                if (not num_episodes) and t == self.batch_size:\n",
        "                    break\n",
        "\n",
        "            path = {\"observation\": np.array(states),\n",
        "                    \"reward\": np.array(rewards),\n",
        "                    \"action\": np.array(actions)}\n",
        "            paths.append(path)\n",
        "            episode += 1\n",
        "            if num_episodes and episode >= num_episodes:\n",
        "                break\n",
        "        return paths, episode_rewards\n",
        "\n",
        "    # 이렇게 수집된 궤적을 가지고 각 상태의 return을 계산할 수 있다.\n",
        "    # we can technically calculating each return G_t by summing over \n",
        "    # all discounted future rewards from step t to the end of each episode:  O(n*n). \n",
        "    # 여기서는 To reduce it to O(n), here we are using another approach: rolling average.\n",
        "    # 1. Essentially for each episode we start with the last state whose return G_t = r_t, \n",
        "    # 2. and calculate the returns in a reversed order to utilize the relationship: \n",
        "    # 3. G_t = r_t + gamma * G_t+1.\n",
        "\n",
        "    # Once we have data from all episodes, \n",
        "    # we flatten the returns to batch these episodic trajectories. \n",
        "    # Earlier in the update() function of the baseline network, \n",
        "    # we have an input parameter target , which is exactly the returns we calculate here.\n",
        "\n",
        "    def get_returns(self, paths):\n",
        "        all_returns = []\n",
        "        for path in paths:\n",
        "            rewards = path[\"reward\"]\n",
        "            returns = []\n",
        "            reversed_rewards = np.flip(rewards,0)\n",
        "            g_t = 0\n",
        "            for r in reversed_rewards:\n",
        "                g_t = r + self.gamma*g_t\n",
        "                returns.insert(0, g_t)\n",
        "            all_returns.append(returns)\n",
        "        returns = np.concatenate(all_returns)\n",
        "        return returns\n",
        "\n",
        "    # With the forecasted values V(s) from the baseline network, \n",
        "    # and the empirical returns, we can also get the advantages.\n",
        "    def get_advantage(self, returns, observations):\n",
        "        values = self.baseline_net.forward(observations).numpy()\n",
        "        advantages = returns - values\n",
        "        advantages = (advantages-np.mean(advantages)) / np.sqrt(np.sum(advantages**2))\n",
        "        return advantages\n",
        "\n",
        "    # Finally, we have all the pieces needed to update the policy network. \n",
        "    # Remember in policy gradient, \n",
        "    # the goal is to maximize the value we obtained by following the policy, \n",
        "    # which is equivalent to minimizing the negative values (loss).\n",
        "    def update_policy(self, observations, actions, advantages):\n",
        "        observations = tf.convert_to_tensor(observations)\n",
        "        actions = tf.convert_to_tensor(actions)\n",
        "        advantages = tf.convert_to_tensor(advantages)\n",
        "        with tf.GradientTape() as tape:\n",
        "            log_prob = self.policy_net.action_distribution(observations).log_prob(actions)\n",
        "            loss = -tf.math.reduce_mean(log_prob * tf.cast(advantages, tf.float32))\n",
        "        grads = tape.gradient(loss, self.policy_net.model.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.policy_net.model.trainable_weights))\n",
        "\n",
        "    # Let’s put everything together and train our model. \n",
        "    # The complete training logic is implemented in train(), \n",
        "    # where in each iteration, we repeat the procedure: \n",
        "    # call play_game() to get several episodic trajectories; \n",
        "    # flatten the trajectories(list of lists) into a batch(list); \n",
        "    # calculate returns and advantages using the batch data; \n",
        "    # update both the baseline network and policy network.\n",
        "    def train(self):\n",
        "        all_total_rewards = []\n",
        "        averaged_total_rewards = []\n",
        "        for t in range(self.num_iterations):\n",
        "            paths, total_rewards = self.play_games()\n",
        "            all_total_rewards.extend(total_rewards)\n",
        "            observations = np.concatenate([path[\"observation\"] for path in paths])\n",
        "            actions = np.concatenate([path[\"action\"] for path in paths])\n",
        "            returns = self.get_returns(paths)\n",
        "            advantages = self.get_advantage(returns, observations)\n",
        "            self.baseline_net.update(observations=observations, target=returns)\n",
        "            self.update_policy(observations, actions, advantages)\n",
        "            avg_reward = np.mean(total_rewards)\n",
        "            averaged_total_rewards.append(avg_reward)\n",
        "            print(\"Average reward for batch {}: {:04.2f}\".format(t,avg_reward))\n",
        "        print(\"Training complete\")\n",
        "        np.save(self.output_path+ \"rewards.npy\", averaged_total_rewards)\n",
        "        export_plot(averaged_total_rewards, \"Reward\", \"CartPole-v0\", self.output_path + \"rewards.png\")\n",
        "\n",
        "    # There are 2 more handy functions helping us evaluate the policy gradient model \n",
        "    # by making a video of its performance on the CartPole environment.\n",
        "    def eval(self, env, num_episodes=1):\n",
        "        paths, rewards = self.play_games(env, num_episodes)\n",
        "        avg_reward = np.mean(rewards)\n",
        "        print(\"Average eval reward: {:04.2f}\".format(avg_reward))\n",
        "        return avg_reward\n",
        "\n",
        "    def make_video(self):\n",
        "        env = wrappers.Monitor(self.env, self.output_path+\"videos\", force=True)\n",
        "        self.eval(env=env, num_episodes=1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn6x1MPbUKsR"
      },
      "source": [
        "Let’s run the code and render a video once training is done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh82m1n-Ql7B"
      },
      "source": [
        "import gym\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = gym.make(\"CartPole-v0\")\n",
        "    model = PolicyGradient(env)\n",
        "    model.train()\n",
        "    #model.make_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVu97vNhUNMp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}